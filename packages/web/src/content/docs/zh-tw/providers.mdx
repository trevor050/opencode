---
title: 供應商
description: 使用 opencode 中的任何 LLM 提供商。
---

import config from "../../../../config.mjs"
export const console = config.console

opencode 使用[人工智能軟件開發工具包](https://ai-sdk.dev/) 和[模型.dev](https://models.dev) 來支持**75+ LLM 提供商**，並且它支持運行本地模型。

要添加提供商，您需要：

1. 使用 `/connect` 命令添加提供程序的 API 密鑰。
2. 在 opencode 配置中配置提供程序。

---

### 證書

當您使用 `/connect` 命令添加提供商的 API 密鑰時，它們會被存儲
在`~/.local/share/opencode/auth.json`。

---

### 配置

您可以通過 opencode 中的 `provider` 部分自定義提供程序
配置。

---

#### 基本網址

您可以通過設置 `baseURL` 選項來自定義任何提供程序的基本 URL。這在使用代理服務或自定義端點時非常有用。

```json title="opencode.json" {6}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "anthropic": {
      "options": {
        "baseURL": "https://api.anthropic.com/v1"
      }
    }
  }
}
```

---

## OpenCode Zen

OpenCode Zen 是 opencode 團隊提供的模型列表，這些模型已被
經過測試和驗證，可以與 opencode 良好配合。 [了解更多](/docs/zen)。

:::tip
如果您是新手，我們建議您從 OpenCode Zen 開始。
:::

1. 在 TUI 中運行 `/connect` 命令，選擇 opencode，然後前往 [opencode.ai/auth](https://opencode.ai/auth)。

   ```txt
   /connect
   ```

2. 登錄，添加您的賬單詳細信息，然後復制您的 API 密鑰。

3. 粘貼您的 API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 在 TUI 中運行 `/models` 以查看我們推薦的型號列表。

   ```txt
   /models
   ```

它的工作方式與 opencode 中的任何其他提供程序一樣，並且完全可以選擇使用。

---

## 目錄

讓我們詳細了解一些提供商。如果您想將提供商添加到
列表，請隨時打開 PR。

:::note
在這裡沒有看到提供商？提交 PR。
:::

---

### 302.艾伊

1. 前往[302.AI控制台](https://302.ai/)，創建一個帳戶並生成一個 API 密鑰。

2. 運行`/connect`命令並蒐索**302.AI**。

   ```txt
   /connect
   ```

3. 輸入您的 302.AI API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行`/models`命令選擇型號。

   ```txt
   /models
   ```

---

### 亞馬遜基岩

要將 Amazon Bedrock 與 opencode 結合使用：

1. 前往 Amazon Bedrock 控制台中的 **模型目錄** 並請求
   訪問您想要的模型。

   :::tip
   您需要能夠在 Amazon Bedrock 中訪問所需的模型。
   :::

2. **使用以下方法之一配置身份驗證**：

   #### 環境變量（快速啟動）

   運行 opencode 時設置以下環境變量之一：

   ```bash
   # Option 1: Using AWS access keys
   AWS_ACCESS_KEY_ID=XXX AWS_SECRET_ACCESS_KEY=YYY opencode

   # Option 2: Using named AWS profile
   AWS_PROFILE=my-profile opencode

   # Option 3: Using Bedrock bearer token
   AWS_BEARER_TOKEN_BEDROCK=XXX opencode
   ```

   或者將它們添加到您的 bash 配置文件中：

   ```bash title="~/.bash_profile"
   export AWS_PROFILE=my-dev-profile
   export AWS_REGION=us-east-1
   ```

   #### 配置文件（推薦）

   對於特定於項目或持久的配置，請使用 `opencode.json`：

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "amazon-bedrock": {
         "options": {
           "region": "us-east-1",
           "profile": "my-aws-profile"
         }
       }
     }
   }
   ```

   **可用選項：**
   - `region` - AWS 區域（例如`us-east-1`、`eu-west-1`）
   - `profile` - 來自 `~/.aws/credentials` 的 AWS 命名配置文件
   - `endpoint` - VPC 終端節點的自定義終端節點 URL（通用 `baseURL` 選項的別名）

   :::tip
   配置文件選項優先於環境變量。
   :::

   #### 高級：VPC 端點

   如果您使用 Bedrock 的 VPC 終端節點：

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "amazon-bedrock": {
         "options": {
           "region": "us-east-1",
           "profile": "production",
           "endpoint": "https://bedrock-runtime.us-east-1.vpce-xxxxx.amazonaws.com"
         }
       }
     }
   }
   ```

   :::note
   `endpoint` 選項是通用 `baseURL` 選項的別名，使用 AWS 特定術語。如果同時指定`endpoint` 和`baseURL`，則`endpoint` 優先。
   :::

   #### 認證方式
   - **`AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY`**：創建 IAM 用戶並在 AWS 控制台中生成訪問密鑰
   - **`AWS_PROFILE`**：使用`~/.aws/credentials` 中的命名配置文件。首先配置`aws configure --profile my-profile`或`aws sso login`
   - **`AWS_BEARER_TOKEN_BEDROCK`**：從 Amazon Bedrock 控制台生成長期 API 密鑰
   - **`AWS_WEB_IDENTITY_TOKEN_FILE` / `AWS_ROLE_ARN`**：適用於 EKS IRSA（服務賬戶的 IAM 角色）或具有 OIDC 聯合的其他 Kubernetes 環境。使用服務帳戶註釋時，這些環境變量由 Kubernetes 自動注入。

   #### 認證優先級

   Amazon Bedrock 使用以下身份驗證優先級：
   1. **不記名令牌** - `AWS_BEARER_TOKEN_BEDROCK` 環境變量或來自 `/connect` 命令的令牌
   2. **AWS 憑證鏈** - 配置文件、訪問密鑰、共享憑證、IAM 角色、Web 身份令牌 (EKS IRSA)、實例元數據

   :::note
   設置不記名令牌（通過 `/connect` 或 `AWS_BEARER_TOKEN_BEDROCK`）時，它優先於所有 AWS 憑證方法（包括配置的配置文件）。
   :::

3. 運行`/models`命令選擇所需的型號。

   ```txt
   /models
   ```

:::note
對於自定義推理配置文件，請在密鑰中使用模型和提供程序名稱，並將 `id` 屬性設置為 arn。這確保了正確的緩存：

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "amazon-bedrock": {
      // ...
      "models": {
        "anthropic-claude-sonnet-4.5": {
          "id": "arn:aws:bedrock:us-east-1:xxx:application-inference-profile/yyy"
        }
      }
    }
  }
}
```

:::

---

### 人擇

1. 註冊後，運行 `/connect` 命令並選擇 Anthropic。

   ```txt
   /connect
   ```

2. 在這裡您可以選擇 **Claude Pro/Max** 選項，它將打開您的瀏覽器
   並要求您進行身份驗證。

   ```txt
   ┌ Select auth method
   │
   │ Claude Pro/Max
   │ Create an API Key
   │ Manually enter API Key
   └
   ```

3. 現在，當您使用 `/models` 命令時，所有 Anthropic 模型都應該可用。

   ```txt
   /models
   ```

:::info
[人擇](https://anthropic.com) 不正式支持在 opencode 中使用您的 Claude Pro/Max 訂閱。
:::

##### 使用 API 密鑰

如果您沒有 Pro/Max 訂閱，您還可以選擇 **創建 API 密鑰**。它還會打開您的瀏覽器並要求您登錄 Anthropic 並為您提供一個可以粘貼到終端中的代碼。

或者，如果您已有 API 密鑰，則可以選擇 **手動輸入 API 密鑰** 並將其粘貼到您的終端中。

---

### Azure 開放人工智能

:::note
如果遇到“抱歉，但我無法協助該請求”錯誤，請嘗試將 Azure 資源中的內容篩選器從 **DefaultV2** 更改為 **Default**。
:::

1. 轉到 [Azure 門戶](https://portal.azure.com/) 並創建 **Azure OpenAI** 資源。你需要：
   - **資源名稱**：這將成為您的 API 端點 (`https://RESOURCE_NAME.openai.azure.com/`) 的一部分
   - **API 密鑰**：來自您的資源的 `KEY 1` 或 `KEY 2`

2. 轉到[Azure 人工智能鑄造廠](https://ai.azure.com/) 並部署模型。

   :::note
   部署名稱必須與模型名稱匹配，opencode 才能正常工作。
   :::

3. 運行 `/connect` 命令並蒐索 **Azure**。

   ```txt
   /connect
   ```

4. 輸入您的 API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

5. 將您的資源名稱設置為環境變量：

   ```bash
   AZURE_RESOURCE_NAME=XXX opencode
   ```

   或者將其添加到您的 bash 配置文件中：

   ```bash title="~/.bash_profile"
   export AZURE_RESOURCE_NAME=XXX
   ```

6. 運行 `/models` 命令以選擇您部署的模型。

   ```txt
   /models
   ```

---

### Azure 認知服務

1. 轉到 [Azure 門戶](https://portal.azure.com/) 並創建 **Azure OpenAI** 資源。你需要：
   - **資源名稱**：這將成為您的 API 端點 (`https://AZURE_COGNITIVE_SERVICES_RESOURCE_NAME.cognitiveservices.azure.com/`) 的一部分
   - **API 密鑰**：來自您的資源的 `KEY 1` 或 `KEY 2`

2. 轉到[Azure 人工智能鑄造廠](https://ai.azure.com/) 並部署模型。

   :::note
   部署名稱必須與模型名稱匹配，opencode 才能正常工作。
   :::

3. 運行 `/connect` 命令並蒐索 **Azure 認知服務**。

   ```txt
   /connect
   ```

4. 輸入您的 API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

5. 將您的資源名稱設置為環境變量：

   ```bash
   AZURE_COGNITIVE_SERVICES_RESOURCE_NAME=XXX opencode
   ```

   或者將其添加到您的 bash 配置文件中：

   ```bash title="~/.bash_profile"
   export AZURE_COGNITIVE_SERVICES_RESOURCE_NAME=XXX
   ```

6. 運行 `/models` 命令以選擇您部署的模型。

   ```txt
   /models
   ```

---

### 巴吉度獵犬

1. 前往[巴吉度獵犬](https://app.baseten.co/)，創建一個帳戶並生成一個 API 密鑰。

2. 運行`/connect`命令並蒐索**Baseten**。

   ```txt
   /connect
   ```

3. 輸入您的 Baseten API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行`/models`命令選擇型號。

   ```txt
   /models
   ```

---

### 大腦

1. 前往[大腦控制台](https://inference.cerebras.ai/)，創建一個帳戶並生成一個 API 密鑰。

2. 運行 `/connect` 命令並蒐索 **Cerebras**。

   ```txt
   /connect
   ```

3. 輸入您的 Cerebras API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇型號，如*Qwen 3 Coder 480B*。

   ```txt
   /models
   ```

---

### Cloudflare AI 網關

Cloudflare AI Gateway 讓您可以通過統一端點訪問來自 OpenAI、Anthropic、Workers AI 等的模型。使用[統一計費](https://developers.cloudflare.com/ai-gateway/features/unified-billing/)，您不需要為每個提供商提供單獨的 API 密鑰。

1. 前往[Cloudflare 儀表板](https://dash.cloudflare.com/)，導航至 **AI** > **AI Gateway**，然後創建一個新網關。

2. 將您的帳戶 ID 和網關 ID 設置為環境變量。

   ```bash title="~/.bash_profile"
   export CLOUDFLARE_ACCOUNT_ID=your-32-character-account-id
   export CLOUDFLARE_GATEWAY_ID=your-gateway-id
   ```

3. 運行 `/connect` 命令並蒐索 **Cloudflare AI Gateway**。

   ```txt
   /connect
   ```

4. 輸入您的 Cloudflare API 令牌。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

   或者將其設置為環境變量。

   ```bash title="~/.bash_profile"
   export CLOUDFLARE_API_TOKEN=your-api-token
   ```

5. 執行`/models`命令選擇型號。

   ```txt
   /models
   ```

   您還可以通過 opencode 配置添加模型。

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "cloudflare-ai-gateway": {
         "models": {
           "openai/gpt-4o": {},
           "anthropic/claude-sonnet-4": {}
         }
       }
     }
   }
   ```

---

### 皮質

1. 前往[Cortecs 控制台](https://cortecs.ai/)，創建一個帳戶並生成一個 API 密鑰。

2. 運行 `/connect` 命令並蒐索 **Cortecs**。

   ```txt
   /connect
   ```

3. 輸入您的 Cortecs API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇一個模型，如*Kimi K2 Instruct*。

   ```txt
   /models
   ```

---

### 深度搜索

1. 前往[DeepSeek 控制台](https://platform.deepseek.com/)，創建一個帳戶，然後單擊“**創建新的 API 密鑰**”。

2. 運行`/connect`命令並蒐索**DeepSeek**。

   ```txt
   /connect
   ```

3. 輸入您的 DeepSeek API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行 `/models` 命令以選擇 DeepSeek 模型，例如 _DeepSeek Reasoner_。

   ```txt
   /models
   ```

---

### 深層基礎設施

1. 前往[深度基礎設施儀表板](https://deepinfra.com/dash)，創建一個帳戶並生成一個 API 密鑰。

2. 運行 `/connect` 命令並蒐索 **Deep Infra**。

   ```txt
   /connect
   ```

3. 輸入您的 Deep Infra API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行`/models`命令選擇型號。

   ```txt
   /models
   ```

---

### 韌體

1. 前往[固件儀表板](https://app.firmware.ai/signup)，創建一個帳戶並生成一個 API 密鑰。

2. 運行`/connect`命令並蒐索**固件**。

   ```txt
   /connect
   ```

3. 輸入您的固件 API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行`/models`命令選擇型號。

   ```txt
   /models
   ```

---

### 煙花人工智能

1. 前往[Fireworks AI 控制台](https://app.fireworks.ai/)，創建一個帳戶，然後單擊“**創建 API 密鑰**”。

2. 運行 `/connect` 命令並蒐索 **Fireworks AI**。

   ```txt
   /connect
   ```

3. 輸入您的 Fireworks AI API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇一個模型，如*Kimi K2 Instruct*。

   ```txt
   /models
   ```

---

### 亞搏體育app二人組

GitLab Duo 通過 GitLab 的 Anthropic 代理提供具有本機工具調用功能的 AI 驅動的代理聊天。

1. 運行 `/connect` 命令並選擇 GitLab。

   ```txt
   /connect
   ```

2. 選擇您的身份驗證方法：

   ```txt
   ┌ Select auth method
   │
   │ OAuth (Recommended)
   │ Personal Access Token
   └
   ```

   #### 使用 OAuth（推薦）

   選擇**OAuth**，您的瀏覽器將打開以進行授權。

   #### 使用個人訪問令牌
   1. 前往[GitLab 用戶設置 > 訪問令牌](https://gitlab.com/-/user_settings/personal_access_tokens)
   2. 單擊**添加新令牌**
   3. 名稱：`OpenCode`，範圍：`api`
   4. 複製令牌（以`glpat-`開頭）
   5. 在終端中輸入

3. 運行 `/models` 命令以查看可用模型。

   ```txt
   /models
   ```

   提供三種基於 Claude 的模型：
   - **duo-chat-haiku-4-5**（默認）- 快速任務的快速響應
   - **duo-chat-sonnet-4-5** - 大多數工作流程的平衡性能
   - **duo-chat-opus-4-5** - 最有能力進行複雜分析

:::note
如果您不想，也可以指定“GITLAB_TOKEN”環境變量
將令牌存儲在 opencode auth 存儲中。
:::

##### 自託管 GitLab

:::note[compliance 筆記]
opencode 使用小型模型來執行某些 AI 任務，例如生成會話標題。
默認情況下，它配置為使用 gpt-5-nano，由 Zen 託管。鎖定 opencode
要僅使用您自己的 GitLab 託管實例，請將以下內容添加到您的
`opencode.json` 文件。還建議禁用會話共享。

```json
{
  "$schema": "https://opencode.ai/config.json",
  "small_model": "gitlab/duo-chat-haiku-4-5",
  "share": "disabled"
}
```

:::

對於自託管的 GitLab 實例：

```bash
export GITLAB_INSTANCE_URL=https://gitlab.company.com
export GITLAB_TOKEN=glpat-...
```

如果您的實例運行自定義 AI 網關：

```bash
GITLAB_AI_GATEWAY_URL=https://ai-gateway.company.com
```

或者添加到您的 bash 配置文件中：

```bash title="~/.bash_profile"
export GITLAB_INSTANCE_URL=https://gitlab.company.com
export GITLAB_AI_GATEWAY_URL=https://ai-gateway.company.com
export GITLAB_TOKEN=glpat-...
```

:::note
您的 GitLab 管理員必須啟用以下功能：

1. [雙代理平台](https://docs.gitlab.com/user/gitlab_duo/turn_on_off/) 用於用戶、組或實例
2. 功能標誌（通過 Rails 控制台）：
   - `agent_platform_claude_code`
   - `third_party_agents_enabled`
     :::

##### 適用於自託管實例的 OAuth

為了使 Oauth 適用於您的自託管實例，您需要創建
一個新的應用程序（設置→應用程序）
回調 URL `http://127.0.0.1:8080/callback` 和以下範圍：

- api（代表您訪問API）
- read_user（讀取您的個人信息）
- read_repository（允許對存儲庫進行只讀訪問）

然後將應用程序 ID 公開為環境變量：

```bash
export GITLAB_OAUTH_CLIENT_ID=your_application_id_here
```

更多文檔請參見 [opencode-gitlab-auth](https://www.npmjs.com/package/@gitlab/opencode-gitlab-auth) 主頁。

##### 配置

通過`opencode.json`定制：

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "gitlab": {
      "options": {
        "instanceUrl": "https://gitlab.com",
        "featureFlags": {
          "duo_agent_platform_agentic_chat": true,
          "duo_agent_platform": true
        }
      }
    }
  }
}
```

##### GitLab API 工具（可選，但強烈推薦）

要訪問 GitLab 工具（合併請求、問題、管道、CI/CD 等）：

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "plugin": ["@gitlab/opencode-gitlab-plugin"]
}
```

該插件提供全面的 GitLab 存儲庫管理功能，包括 MR 審查、問題跟踪、管道監控等。

---

### GitHub 副駕駛

要將 GitHub Copilot 訂閱與 opencode 結合使用：

:::note
某些型號可能需要 [Pro+
訂閱](https://github.com/features/copilot/plans) 來使用。

某些模型需要在您的[GitHub Copilot 設置](https://docs.github.com/en/copilot/how-tos/use-ai-models/configure-access-to-ai-models#setup-for-individual-use) 中手動啟用。
:::

1. 運行 `/connect` 命令並蒐索 GitHub Copilot。

   ```txt
   /connect
   ```

2. 導航至[github.com/login/device](https://github.com/login/device) 並輸入代碼。

   ```txt
   ┌ Login with GitHub Copilot
   │
   │ https://github.com/login/device
   │
   │ Enter code: 8F43-6FCF
   │
   └ Waiting for authorization...
   ```

3. 現在運行`/models`命令來選擇您想要的型號。

   ```txt
   /models
   ```

---

### 谷歌頂點人工智能

要將 Google Vertex AI 與 opencode 結合使用：

1. 前往 Google Cloud Console 中的 **Model Garden** 並檢查
   您所在地區提供的型號。

   :::note
   您需要有一個啟用了 Vertex AI API 的 Google Cloud 項目。
   :::

2. 設置所需的環境變量：
   - `GOOGLE_CLOUD_PROJECT`：您的 Google Cloud 項目 ID
   - `VERTEX_LOCATION`（可選）：Vertex AI 的區域（默認為`global`）
   - 身份驗證（選擇一項）：
     - `GOOGLE_APPLICATION_CREDENTIALS`: Path to your service account JSON key file
     - Authenticate using gcloud CLI: `gcloud auth application-default login`

   在運行 opencode 時設置它們。

   ```bash
   GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json GOOGLE_CLOUD_PROJECT=your-project-id opencode
   ```

   或者將它們添加到您的 bash 配置文件中。

   ```bash title="~/.bash_profile"
   export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
   export GOOGLE_CLOUD_PROJECT=your-project-id
   export VERTEX_LOCATION=global
   ```

:::tip
`global` 區域無需額外成本即可提高可用性並減少錯誤。使用區域端點（例如`us-central1`）來滿足數據駐留要求。 [了解更多](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models#regional_and_global_endpoints)
:::

3. 運行`/models`命令選擇所需的型號。

   ```txt
   /models
   ```

---

### 格羅克

1. 前往[格羅克控制台](https://console.groq.com/)，單擊“**創建 API 密鑰**”，然後復制密鑰。

2. 運行`/connect`命令並蒐索Groq。

   ```txt
   /connect
   ```

3. 輸入提供商的 API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇您想要的。

   ```txt
   /models
   ```

---

### 抱臉

[Error 500 (Server Error)!!1500.That’s an error.There was an error. Please try again later.That’s all we know.](https://huggingface.co/docs/inference-providers) 提供對超過 17 個提供商支持的開放模型的訪問。

1. 前往[擁抱臉部設置](https://huggingface.co/settings/tokens/new?ownUserPermissions=inference.serverless.write&tokenType=fineGrained) 創建一個具有調用推理提供程序權限的令牌。

2. 運行 `/connect` 命令並蒐索 **Hugging Face**。

   ```txt
   /connect
   ```

3. 輸入您的擁抱臉標記。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行 `/models` 命令來選擇模型，如 _Kimi-K2-Instruct_ 或 _GLM-4.6_。

   ```txt
   /models
   ```

---

### 螺旋錐

[螺旋錐](https://helicone.ai) 是一個 LLM 可觀察性平台，可為您的 AI 應用程序提供日誌記錄、監控和分析。 Helicone AI Gateway 根據模型自動將您的請求路由到適當的提供商。

1. 前往[螺旋錐](https://helicone.ai)，創建一個帳戶，並從您的儀表板生成 API 密鑰。

2. 運行 `/connect` 命令並蒐索 **Helicone**。

   ```txt
   /connect
   ```

3. 輸入您的 Helicone API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行`/models`命令選擇型號。

   ```txt
   /models
   ```

有關更多提供程序和高級功能（例如緩存和速率限制），請查看[螺旋錐文檔](https://docs.helicone.ai)。

#### 可選配置

如果您發現 Helicone 的功能或模型未通過opencode自動配置，您始終可以自行配置。

這是[Helicone 的模型目錄](https://helicone.ai/models)，您將需要它來獲取要添加的模型的 ID。

```jsonc title="~/.config/opencode/opencode.jsonc"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "helicone": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Helicone",
      "options": {
        "baseURL": "https://ai-gateway.helicone.ai",
      },
      "models": {
        "gpt-4o": {
          // Model ID (from Helicone's model directory page)
          "name": "GPT-4o", // Your own custom name for the model
        },
        "claude-sonnet-4-20250514": {
          "name": "Claude Sonnet 4",
        },
      },
    },
  },
}
```

#### 自定義標頭

Helicone 支持緩存、用戶跟踪和會話管理等功能的自定義標頭。使用 `options.headers` 將它們添加到您的提供程序配置中：

```jsonc title="~/.config/opencode/opencode.jsonc"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "helicone": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Helicone",
      "options": {
        "baseURL": "https://ai-gateway.helicone.ai",
        "headers": {
          "Helicone-Cache-Enabled": "true",
          "Helicone-User-Id": "opencode",
        },
      },
    },
  },
}
```

##### 會話跟踪

Helicone 的 [會議](https://docs.helicone.ai/features/sessions) 功能可讓您將相關的 LLM 請求分組在一起。使用 [opencode-helicone-會話](https://github.com/H2Shami/opencode-helicone-session) 插件自動將每個 opencode 對話記錄為 Helicone 中的會話。

```bash
npm install -g opencode-helicone-session
```

將其添加到您的配置中。

```json title="opencode.json"
{
  "plugin": ["opencode-helicone-session"]
}
```

該插件將 `Helicone-Session-Id` 和 `Helicone-Session-Name` 標頭注入您的請求中。在 Helicone 的會話頁面中，您將看到每個 opencode 對話都列為單獨的會話。

##### 常見 Helicone 接頭

| 標題                       | 描述                                                  |
| -------------------------- | ----------------------------------------------------- |
| `Helicone-Cache-Enabled`   | 啟用響應緩存 (`true`/`false`)                         |
| `Helicone-User-Id`         | 按用戶跟踪指標                                        |
| `Helicone-Property-[Name]` | 添加自定義屬性（例如`Helicone-Property-Environment`） |
| `Helicone-Prompt-Id`       | 將請求與提示版本相關聯                                |

有關所有可用標頭，請參閱[Helicone 頭目錄](https://docs.helicone.ai/helicone-headers/header-directory)。

---

### 調用.cpp

您可以通過[駱駝.cpp 的](https://github.com/ggml-org/llama.cpp) llama-server實用程序配置opencode以使用本地模型

```json title="opencode.json" "llama.cpp" {5, 6, 8, 10-15}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "llama.cpp": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "llama-server (local)",
      "options": {
        "baseURL": "http://127.0.0.1:8080/v1"
      },
      "models": {
        "qwen3-coder:a3b": {
          "name": "Qwen3-Coder: a3b-30b (local)",
          "limit": {
            "context": 128000,
            "output": 65536
          }
        }
      }
    }
  }
}
```

在這個例子中：

- `llama.cpp` 是自定義提供商 ID。這可以是您想要的任何字符串。
- `npm` 指定用於此提供程序的包。這裡，`@ai-sdk/openai-compatible` 用於任何 OpenAI 兼容的 API。
- `name` 是 UI 中提供程序的顯示名稱。
- `options.baseURL` 是本地服務器的端點。
- `models` 是模型 ID 與其配置的映射。型號名稱將顯示在型號選擇列表中。

---

### IO網絡

IO.NET 提供了 17 種針對各種用例進行優化的模型：

1. 前往[IO.NET控制台](https://ai.io.net/)，創建一個帳戶並生成一個 API 密鑰。

2. 運行`/connect`命令並蒐索**IO.NET**。

   ```txt
   /connect
   ```

3. 輸入您的 IO.NET API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行`/models`命令選擇型號。

   ```txt
   /models
   ```

---

### LM工作室

您可以通過 LM Studio 配置 opencode 以使用本地模型。

```json title="opencode.json" "lmstudio" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "lmstudio": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "LM Studio (local)",
      "options": {
        "baseURL": "http://127.0.0.1:1234/v1"
      },
      "models": {
        "google/gemma-3n-e4b": {
          "name": "Gemma 3n-e4b (local)"
        }
      }
    }
  }
}
```

在這個例子中：

- `lmstudio` 是自定義提供商 ID。這可以是您想要的任何字符串。
- `npm` 指定用於此提供程序的包。這裡，`@ai-sdk/openai-compatible` 用於任何 OpenAI 兼容的 API。
- `name` 是 UI 中提供程序的顯示名稱。
- `options.baseURL` 是本地服務器的端點。
- `models` 是模型 ID 與其配置的映射。型號名稱將顯示在型號選擇列表中。

---

### 登月人工智能

要使用 Moonshot AI 中的 Kimi K2：

1. 前往[Moonshot 人工智能控制台](https://platform.moonshot.ai/console)，創建一個帳戶，然後單擊“**創建 API 密鑰**”。

2. 運行 `/connect` 命令並蒐索 **Moonshot AI**。

   ```txt
   /connect
   ```

3. 輸入您的 Moonshot API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇*Kimi K2*。

   ```txt
   /models
   ```

---

### 最小最大

1. 前往[MiniMax API 控制台](https://platform.minimax.io/login)，創建一個帳戶並生成一個 API 密鑰。

2. 運行 `/connect` 命令並蒐索 **MiniMax**。

   ```txt
   /connect
   ```

3. 輸入您的 MiniMax API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇型號，如*M2.1*。

   ```txt
   /models
   ```

---

### Nebius 代幣工廠

1. 前往[Nebius 令牌工廠控制台](https://tokenfactory.nebius.com/)，創建一個帳戶，然後單擊“**添加密鑰**”。

2. 運行`/connect`命令並蒐索**Nebius令牌工廠**。

   ```txt
   /connect
   ```

3. 輸入您的 Nebius 令牌工廠 API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇一個模型，如*Kimi K2 Instruct*。

   ```txt
   /models
   ```

---

### 成為

您可以通過 Ollama 配置 opencode 以使用本地模型。

:::tip
Ollama 可以自動為 opencode 配置自身。詳情請參閱[Ollama 集成文檔](https://docs.ollama.com/integrations/opencode)。
:::

```json title="opencode.json" "ollama" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "ollama": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Ollama (local)",
      "options": {
        "baseURL": "http://localhost:11434/v1"
      },
      "models": {
        "llama2": {
          "name": "Llama 2"
        }
      }
    }
  }
}
```

在這個例子中：

- `ollama` 是自定義提供商 ID。這可以是您想要的任何字符串。
- `npm` 指定用於此提供程序的包。這裡，`@ai-sdk/openai-compatible` 用於任何 OpenAI 兼容的 API。
- `name` 是 UI 中提供程序的顯示名稱。
- `options.baseURL` 是本地服務器的端點。
- `models` 是模型 ID 與其配置的映射。型號名稱將顯示在型號選擇列表中。

:::tip
如果工具調用不起作用，請嘗試增加 Ollama 中的`num_ctx`。從 16k - 32k 左右開始。
:::

---

### 奧拉馬雲

要將 Ollama Cloud 與 opencode 結合使用：

1. 前往[https://llama.com/](https://ollama.com/) 並登錄或創建帳戶。

2. 導航到 **設置** > **密鑰**，然後單擊 **添加 API 密鑰** 以生成新的 API 密鑰。

3. 複製 API 密鑰以在 opencode 中使用。

4. 運行 `/connect` 命令並蒐索 **Ollama Cloud**。

   ```txt
   /connect
   ```

5. 輸入您的 Ollama Cloud API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

6. **重要**：在opencode中使用雲模型之前，必須將模型信息拉取到本地：

   ```bash
   ollama pull gpt-oss:20b-cloud
   ```

7. 運行 `/models` 命令以選擇您的 Ollama Cloud 型號。

   ```txt
   /models
   ```

---

### 開放人工智能

我們建議註冊[ChatGPT Plus 或 Pro](https://chatgpt.com/pricing)。

1. 註冊後，運行 `/connect` 命令並選擇 OpenAI。

   ```txt
   /connect
   ```

2. 在這裡您可以選擇 **ChatGPT Plus/Pro** 選項，它將打開您的瀏覽器
   並要求您進行身份驗證。

   ```txt
   ┌ Select auth method
   │
   │ ChatGPT Plus/Pro
   │ Manually enter API Key
   └
   ```

3. 現在，當您使用 `/models` 命令時，所有 OpenAI 模型都應該可用。

   ```txt
   /models
   ```

##### 使用 API 密鑰

如果您已有 API 密鑰，可以選擇 **手動輸入 API 密鑰** 並將其粘貼到您的終端中。

---

### OpenCode Zen

OpenCode Zen 是 opencode 團隊提供的經過測試和驗證的模型列表。 [了解更多](/docs/zen)。

1. 登錄 **<a href={console}>OpenCode Zen</a>** 並單擊 **創建 API 密鑰**。

2. 運行 `/connect` 命令並蒐索 **OpenCode Zen**。

   ```txt
   /connect
   ```

3. 輸入您的 opencode API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇型號，如*Qwen 3 Coder 480B*。

   ```txt
   /models
   ```

---

### 開放路由器

1. 前往[OpenRouter儀表板](https://openrouter.ai/settings/keys)，單擊“**創建 API 密鑰**”，然後復制密鑰。

2. 運行`/connect`命令並蒐索OpenRouter。

   ```txt
   /connect
   ```

3. 輸入提供商的 API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. OpenRouter默認預加載了多種型號，運行`/models`命令選擇您想要的型號。

   ```txt
   /models
   ```

   您還可以通過 opencode 配置添加其他模型。

   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "somecoolnewmodel": {}
         }
       }
     }
   }
   ```

5. 您還可以通過 opencode 配置自定義它們。這是指定提供商的示例

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "moonshotai/kimi-k2": {
             "options": {
               "provider": {
                 "order": ["baseten"],
                 "allow_fallbacks": false
               }
             }
           }
         }
       }
     }
   }
   ```

---

### SAP人工智能核心

SAP AI Core 通過統一平台提供對 OpenAI、Anthropic、Google、Amazon、Meta、Mistral 和 AI21 的 40 多個模型的訪問。

1. 轉到[SAP BTP 駕駛艙](https://account.hana.ondemand.com/)，導航到 SAP AI Core 服務實例，然後創建服務密鑰。

   :::tip
   服務密鑰是一個包含`clientid`、`clientsecret`、`url` 和`serviceurls.AI_API_URL` 的JSON 對象。您可以在 BTP Cockpit 的 **服務** > **實例和訂閱** 下找到您的 AI Core 實例。
   :::

2. 運行 `/connect` 命令並蒐索 **SAP AI Core**。

   ```txt
   /connect
   ```

3. 輸入您的服務密鑰 JSON。

   ```txt
   ┌ Service key
   │
   │
   └ enter
   ```

   或者設置`AICORE_SERVICE_KEY`環境變量：

   ```bash
   AICORE_SERVICE_KEY='{"clientid":"...","clientsecret":"...","url":"...","serviceurls":{"AI_API_URL":"..."}}' opencode
   ```

   或者將其添加到您的 bash 配置文件中：

   ```bash title="~/.bash_profile"
   export AICORE_SERVICE_KEY='{"clientid":"...","clientsecret":"...","url":"...","serviceurls":{"AI_API_URL":"..."}}'
   ```

4. （可選）設置部署 ID 和資源組：

   ```bash
   AICORE_DEPLOYMENT_ID=your-deployment-id AICORE_RESOURCE_GROUP=your-resource-group opencode
   ```

   :::note
   這些設置是可選的，應根據您的 SAP AI Core 設置進行配置。
   :::

5. 運行 `/models` 命令從 40 多個可用型號中進行選擇。

   ```txt
   /models
   ```

---

### OVHcloud AI 端點

1. 前往[OVH雲面板](https://ovh.com/manager)。導航到 `Public Cloud` 部分，`AI & Machine Learning` > `AI Endpoints`，然後在 `API Keys` 選項卡中單擊 **創建新的 API 密鑰**。

2. 運行 `/connect` 命令並蒐索 **OVHcloud AI Endpoints**。

   ```txt
   /connect
   ```

3. 輸入您的 OVHcloud AI Endpoints API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇型號，如*gpt-oss-120b*。

   ```txt
   /models
   ```

---

### 斯卡威

要將 [Scaleway 生成 API](https://www.scaleway.com/en/docs/generative-apis/) 與 opencode 一起使用：

1. 前往[Scaleway 控制台 IAM 設置](https://console.scaleway.com/iam/api-keys) 生成新的 API 密鑰。

2. 運行 `/connect` 命令並蒐索 **Scaleway**。

   ```txt
   /connect
   ```

3. 輸入您的 Scaleway API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇模型，如*devstral-2-123b-instruct-2512*或*gpt-oss-120b*。

   ```txt
   /models
   ```

---

### 一起人工智能

1. 前往[一起AI控制台](https://api.together.ai)，創建一個帳戶，然後單擊“**添加密鑰**”。

2. 運行 `/connect` 命令並蒐索 **Together AI**。

   ```txt
   /connect
   ```

3. 輸入您的 Together AI API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇一個模型，如*Kimi K2 Instruct*。

   ```txt
   /models
   ```

---

### 威尼斯人工智能

1. 前往[威尼斯人工智能控制台](https://venice.ai)，創建一個帳戶並生成一個 API 密鑰。

2. 運行`/connect`命令並蒐索**Venice AI**。

   ```txt
   /connect
   ```

3. 輸入您的 Venice AI API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇型號，如*Llama 3.3 70B*。

   ```txt
   /models
   ```

---

### Vercel人工智能網關

Vercel AI Gateway 可讓您通過統一端點訪問來自 OpenAI、Anthropic、Google、xAI 等的模型。型號按標價提供，不加價。

1. 前往[維塞爾儀表板](https://vercel.com/)，導航至 **AI Gateway** 選項卡，然後單擊 **API 密鑰** 以創建新的 API 密鑰。

2. 運行 `/connect` 命令並蒐索 **Vercel AI Gateway**。

   ```txt
   /connect
   ```

3. 輸入您的 Vercel AI Gateway API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行`/models`命令選擇型號。

   ```txt
   /models
   ```

您還可以通過 opencode 配置自定義模型。以下是指定提供者路由順序的示例。

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "vercel": {
      "models": {
        "anthropic/claude-sonnet-4": {
          "options": {
            "order": ["anthropic", "vertex"]
          }
        }
      }
    }
  }
}
```

一些有用的路由選項：

| 選項                | 描述                         |
| ------------------- | ---------------------------- |
| `order`             | 提供者嘗試順序               |
| `only`              | 限制特定提供商               |
| `zeroDataRetention` | 僅使用零數據保留政策的提供商 |

---

### 人工智慧

1. 前往[xAI控制台](https://console.x.ai/)，創建一個帳戶並生成一個 API 密鑰。

2. 運行`/connect`命令並蒐索**xAI**。

   ```txt
   /connect
   ```

3. 輸入您的 xAI API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇一個模型，如*Grok Beta*。

   ```txt
   /models
   ```

---

### 扎艾

1. 前往[Z.AI API控制台](https://z.ai/manage-apikey/apikey-list)，創建一個帳戶，然後單擊“**創建新的 API 密鑰**”。

2. 運行`/connect`命令並蒐索**Z.AI**。

   ```txt
   /connect
   ```

   如果您訂閱了 **GLM 編碼計劃**，請選擇 **Z.AI 編碼計劃**。

3. 輸入您的 Z.AI API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 運行`/models`命令選擇*GLM-4.7*等模型。

   ```txt
   /models
   ```

---

### 多路復用器

1. 前往[ZenMux 儀表板](https://zenmux.ai/settings/keys)，單擊“**創建 API 密鑰**”，然後復制密鑰。

2. 運行`/connect`命令並蒐索ZenMux。

   ```txt
   /connect
   ```

3. 輸入提供商的 API 密鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 默認情況下預加載了許多 ZenMux 模型，運行 `/models` 命令選擇您想要的模型。

   ```txt
   /models
   ```

   您還可以通過 opencode 配置添加其他模型。

   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "zenmux": {
         "models": {
           "somecoolnewmodel": {}
         }
       }
     }
   }
   ```

---

## 定制提供商

要添加 `/connect` 命令中未列出的任何 **OpenAI 兼容** 提供程序：

:::tip
您可以將任何與 OpenAI 兼容的提供程序與 opencode 結合使用。大多數現代人工智能提供商都提供與 OpenAI 兼容的 API。
:::

1. 運行 `/connect` 命令並向下滾動到 **其他**。

   ```bash
   $ /connect

   ┌  Add credential
   │
   ◆  Select provider
   │  ...
   │  ● Other
   └
   ```

2. 輸入提供商的唯一 ID。

   ```bash
   $ /connect

   ┌  Add credential
   │
   ◇  Enter provider id
   │  myprovider
   └
   ```

   :::note
   選擇一個容易記住的 ID，您將在配置文件中使用它。
   :::

3. 輸入提供商的 API 密鑰。

   ```bash
   $ /connect

   ┌  Add credential
   │
   ▲  This only stores a credential for myprovider - you will need to configure it in opencode.json, check the docs for examples.
   │
   ◇  Enter your API key
   │  sk-...
   └
   ```

4. 在項目目錄中創建或更新 `opencode.json` 文件：

   ```json title="opencode.json" ""myprovider"" {5-15}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "myprovider": {
         "npm": "@ai-sdk/openai-compatible",
         "name": "My AI ProviderDisplay Name",
         "options": {
           "baseURL": "https://api.myprovider.com/v1"
         },
         "models": {
           "my-model-name": {
             "name": "My Model Display Name"
           }
         }
       }
     }
   }
   ```

   以下是配置選項：
   - **npm**：要使用的 AI SDK 包，`@ai-sdk/openai-compatible` 用於 OpenAI 兼容提供商
   - **名稱**：UI 中的顯示名稱。
   - **型號**：可用型號。
   - **options.baseURL**：API 端點 URL。
   - **options.apiKey**：如果不使用身份驗證，可以選擇設置 API 密鑰。
   - **options.headers**：可選擇設置自定義標頭。

   有關高級選項的更多信息，請參見下面的示例。

5. 運行 `/models` 命令，您的自定義提供程序和模型將出現在選擇列表中。

---

##### 例子

以下是設置`apiKey`、`headers` 和模型`limit` 選項的示例。

```json title="opencode.json" {9,11,17-20}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "myprovider": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "My AI ProviderDisplay Name",
      "options": {
        "baseURL": "https://api.myprovider.com/v1",
        "apiKey": "{env:ANTHROPIC_API_KEY}",
        "headers": {
          "Authorization": "Bearer custom-token"
        }
      },
      "models": {
        "my-model-name": {
          "name": "My Model Display Name",
          "limit": {
            "context": 200000,
            "output": 65536
          }
        }
      }
    }
  }
}
```

配置詳情：

- **apiKey**：使用`env`變量語法[了解更多](/docs/config#env-vars)設置。
- ** headers **：隨每個請求發送的自定義標頭。
- **limit.context**：模型接受的最大輸入標記。
- **limit.output**：模型可以生成的最大令牌。

`limit` 字段允許 opencode 了解您還剩下多少上下文。標準提供商會自動從 models.dev 中提取這些內容。

---

## 故障排除

如果您在配置提供商時遇到問題，請檢查以下內容：

1. **檢查身份驗證設置**：運行 `opencode auth list` 以查看憑據是否
   提供商的配置已添加到您的配置中。

   這不適用於 Amazon Bedrock 等依賴環境變量進行身份驗證的提供商。

2. 對於自定義提供程序，請檢查 opencode 配置並：
   - 確保 `/connect` 命令中使用的提供程序 ID 與 opencode 配置中的 ID 匹配。
   - 正確的 npm 包用於提供程序。例如，對 Cerebras 使用 `@ai-sdk/cerebras`。對於所有其他 OpenAI 兼容提供商，請使用 `@ai-sdk/openai-compatible`。
   - 檢查 `options.baseURL` 字段中使用了正確的 API 端點。
